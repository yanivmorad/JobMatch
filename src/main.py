import json
import os
import time

from tabulate import tabulate

from engine import JobAnalyzer  # ×•×•×“× ×©×”×§×œ××¡ ×‘×§×•×‘×¥ engine.py × ×§×¨× JobAnalyzer
from scraper import Scraper  # ×•×•×“× ×©×”×§×œ××¡ ×‘×§×•×‘×¥ scraper.py × ×§×¨× Scraper

# ×”×’×“×¨×ª × ×ª×™×‘×™×
BASE_DIR = os.path.dirname(os.path.dirname(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
RESUME_PATH = os.path.join(DATA_DIR, "resume.txt")
CONTEXT_PATH = os.path.join(DATA_DIR, "Personal Context.txt")
JOBS_LIST_PATH = os.path.join(DATA_DIR, "jobs.txt")
RESULTS_PATH = os.path.join(BASE_DIR, "results.json")


def load_file(path):
    if not os.path.exists(path):
        print(f"âš ï¸ Warning: {path} not found.")
        return ""
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def load_jobs_list(path):
    if not os.path.exists(path):
        return []
    with open(path, "r", encoding="utf-8") as f:
        return [line.strip() for line in f if line.strip() and not line.startswith("#")]


def main():
    print("\nğŸš€ --- JobMatch System Starting ---")

    # ×˜×¢×™× ×ª × ×ª×•× ×™×
    resume = load_file(RESUME_PATH)
    context = load_file(CONTEXT_PATH)
    entries = load_jobs_list(JOBS_LIST_PATH)

    if not resume:
        print("âŒ Error: Please fill in data/resume.txt")
        return

    if not entries:
        print(f"âŒ Error: No entries found in {JOBS_LIST_PATH}.")
        return

    print(f"âœ… Loaded resume and context. Found {len(entries)} jobs to process.")

    # ××ª×—×•×œ ×¨×›×™×‘×™×
    scraper = Scraper()
    analyzer = JobAnalyzer()
    results = []

    for entry in entries:
        job_data = None
        display_url = entry[:50] + "..." if len(entry) > 50 else entry

        # 1. ×©×œ×‘ ×”×¡×¨×™×§×” (Scraping)
        if entry.startswith(("http://", "https://")):
            print(f"\nğŸŒ Scrapping: {display_url}")
            job_data = scraper.scrape(entry)
        else:
            print("\nğŸ“ Manual Text Input detected...")
            job_data = {
                "url": "Manual Entry",
                "company": "Pending Analysis",
                "job_title": "Pending Analysis",
                "full_description": entry,
            }

        # 2. ××™××•×ª × ×ª×•× ×™×
        if not job_data or not job_data.get("full_description"):
            print(f"âš ï¸ Skipping: Could not get content for {display_url}")
            continue

        # 3. × ×™×ª×•×— ×¢× Gemini (×›×•×œ×œ ×ª×™×§×•×Ÿ ×›×•×ª×¨×•×ª ×•×¡×™×›×•× ×‘×¢×‘×¨×™×ª)
        print("ğŸ¤– Analyzing & Cleaning Data with Gemini...")
        analysis = analyzer.analyze(resume, context, job_data)

        # ××™×–×•×’ × ×ª×•× ×™×: ×”-AI ×“×•×¨×¡ ××ª ×”× ×ª×•× ×™× ×”×’×•×œ××™×™× ××”×¡×§×¨×™×™×¤×¨ ×‘××™×“×” ×•××¦× ×“×™×•×§ ×˜×•×‘ ×™×•×ª×¨
        final_entry = {
            "url": job_data.get("url"),
            "scraped_at": job_data.get("scraped_at"),
            **analysis,  # ××›×™×œ company, job_title, job_summary_hebrew, suitability_score ×•×›×•'
        }

        results.append(final_entry)

        # 4. ×”××ª× ×” ×œ×× ×™×¢×ª ×—×¡×™××•×ª (Rate Limiting)
        print(
            f"ğŸ“Š Done: {final_entry['job_title']} @ {final_entry['company']} (Score: {final_entry['suitability_score']})"
        )
        print("â³ Waiting before next job...")
        time.sleep(4)

    # ×©××™×¨×ª ×ª×•×¦××•×ª
    if results:
        with open(RESULTS_PATH, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ Results saved to: {RESULTS_PATH}")

        # ×”×“×¤×¡×ª ×˜×‘×œ×” ××¡×›××ª
        table_data = []
        for r in results:
            table_data.append(
                [
                    str(r.get("company"))[:15],
                    str(r.get("job_title"))[:25],
                    f"{r.get('suitability_score')}/100",
                    f"{r.get('acceptance_probability')}%",
                    str(r.get("recommendation"))[:50] + "...",
                ]
            )

        print("\n" + "=" * 90)
        print("ğŸ¯ FINAL JOB MATCH SUMMARY")
        print("=" * 90)
        print(
            tabulate(
                table_data,
                headers=["Company", "Title", "Match", "Prob %", "Bottom Line"],
                tablefmt="fancy_grid",
            )
        )
    else:
        print("\nâŒ No jobs were successfully analyzed.")


if __name__ == "__main__":
    main()
