# Worker URL Resolution & Duplicate Prevention

## Overview
The scrape worker now includes intelligent URL resolution and duplicate detection to prevent wasting resources on URLs that already exist in the database.

## Problem Solved

**Before**: When a HireMeTech URL was added:
1. Worker would scrape the HireMeTech page
2. Resolver would get the actual company URL
3. Content would be saved with the company URL
4. **BUT**: If someone later added the same HireMeTech job again, it would scrape it again, even though the company URL already exists in the DB

**After**: The worker now uses a **two-pass approach**:
1. **Pass 1**: Detects HireMeTech URLs, resolves them, updates the database, and continues to next job
2. **Pass 2**: Worker picks up the updated URL and scrapes it (no resolution needed)

## Workflow

```
PASS 1: Resolution & Database Update
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Job URL: https://hiremetech.com/job/123
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: Detect Special URL            â”‚
â”‚ Is this HireMeTech? â†’ YES              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: Resolve URL                   â”‚
â”‚ Use resolver.resolve()                 â”‚
â”‚ Result: https://greenhouse.io/job/456  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: Check Database                â”‚
â”‚ Does greenhouse.io/job/456 exist?      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â–º YES â†’ Delete duplicate job
         â”‚         Skip to next job
         â”‚
         â””â”€â–º NO  â†’ Update URL in database
                   Set status = WAITING_FOR_SCRAPE
                   Continue to next job
                   (Don't scrape yet!)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PASS 2: Actual Scraping (Next Iteration)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Job URL: https://greenhouse.io/job/456 â† Updated!
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: Detect Special URL            â”‚
â”‚ Is this HireMeTech? â†’ NO               â”‚
â”‚ (Already resolved!)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: Scrape the URL                â”‚
â”‚ - Jina AI (primary)                    â”‚
â”‚ - Playwright (fallback)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: Save to Database               â”‚
â”‚ - full_description                     â”‚
â”‚ - company, job_title                   â”‚
â”‚ - status = WAITING_FOR_AI              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
       âœ… DONE!
```

## Code Changes

### 1. `db/jobs_repository.py`
Added new function:
```python
async def delete_job_by_id(job_id: int):
    """Delete a job by its ID"""
    pool = await get_pool()
    await pool.execute("DELETE FROM jobs WHERE id = $1", job_id)

async def update_job_url(job_id: int, new_url: str):
    """
    Update the URL for a job (e.g., when HireMeTech URL resolves to company URL).
    Also resets status to WAITING_FOR_SCRAPE so the worker picks it up again.
    """
    pool = await get_pool()
    await pool.execute(
        """
        UPDATE jobs 
        SET url = $1, status = 'WAITING_FOR_SCRAPE'
        WHERE id = $2
        """,
        new_url,
        job_id,
    )
```

### 2. `workers/worker_manager.py`
Updated `scrape_worker()` with two-pass logic:

**PASS 1**: Pre-scrape URL resolution
- Detects if URL is from HireMeTech
- Uses `scraper.resolver.resolve()` to get company URL
- Checks for duplicates
- Updates URL in database
- Continues to next job (no scraping yet!)

**PASS 2**: Actual scraping (next iteration)
- Worker picks up the updated URL
- No resolution needed (already done!)
- Scrapes and saves content

## Benefits

âœ… **No Double Resolution**: URL is resolved only once  
âœ… **Prevents Duplicate Scraping**: Checks before any expensive operations  
âœ… **Database Always Accurate**: URL is updated before scraping  
âœ… **Saves Resources**: No wasted API calls or processing time  
âœ… **Cleaner Logs**: Clear separation between resolution and scraping  

## Example Logs

### Scenario 1: New HireMeTech Job (Pass 1 - Resolution)
```
ğŸ•·ï¸ Processing: https://hiremetech.com/job/123
ğŸ” Detected HireMeTech URL, resolving...
âœ… Resolved: https://hiremetech.com/job/123 â†’ https://greenhouse.io/job/456
ğŸ“ Updating job URL in database: https://hiremetech.com/job/123 â†’ https://greenhouse.io/job/456
âœ“ URL updated. Continuing to next job - worker will pick this up again with new URL.
```

### Scenario 2: Same Job (Pass 2 - Scraping)
```
ğŸ•·ï¸ Processing: https://greenhouse.io/job/456
ğŸ•·ï¸ Scraping: https://greenhouse.io/job/456
ğŸ“¡ Scraping via Jina: https://greenhouse.io/job/456
âœ… Scrape complete for: https://greenhouse.io/job/456
```

### Scenario 3: Duplicate Detected
```
ğŸ•·ï¸ Processing: https://hiremetech.com/job/789
ğŸ” Detected HireMeTech URL, resolving...
âœ… Resolved: https://hiremetech.com/job/789 â†’ https://greenhouse.io/job/456
â­ï¸ DUPLICATE DETECTED: Resolved URL 'https://greenhouse.io/job/456' 
   already exists in DB (Job ID: 42). Deleting duplicate job ID 89.
```

## Future Enhancements

This pattern can be extended to other job sites:

```python
# In scrape_worker
if "hiremetech.com" in original_url:
    resolved_url = await asyncio.to_thread(
        scraper.resolver.resolve, original_url
    )
elif "othersite.com" in original_url:
    resolved_url = await asyncio.to_thread(
        scraper.resolver.resolve, original_url
    )
```

Just add the site detection logic to `scraper_service/resolvers.py` and it will work automatically!
